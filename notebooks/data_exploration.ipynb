{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Score Analysis - Data Exploration\n",
    "\n",
    "This notebook explores the datasets used for risk score analysis. We'll examine the structure, distributions, and relationships in the data to inform feature engineering and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, mean, stddev, min, max\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import (\n",
    "    DELTA_TABLES,\n",
    "    NUMERICAL_FEATURES,\n",
    "    CATEGORICAL_FEATURES,\n",
    "    DATE_FEATURES,\n",
    "    TARGET_COLUMN\n",
    ")\n",
    "from src.utils import get_spark_session, read_delta_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Spark Session\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "Let's load the raw data from Delta tables and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read raw data from Delta tables\n",
    "applications_df = read_delta_table(spark, f\"{DELTA_TABLES['raw_data']}/loan_applications\")\n",
    "customers_df = read_delta_table(spark, f\"{DELTA_TABLES['raw_data']}/customers\")\n",
    "performance_df = read_delta_table(spark, f\"{DELTA_TABLES['raw_data']}/loan_performance\")\n",
    "\n",
    "# If tables don't exist yet, print a message\n",
    "if applications_df is None:\n",
    "    print(\"Loan applications table not found. Run the data ingestion pipeline first.\")\n",
    "if customers_df is None:\n",
    "    print(\"Customers table not found. Run the data ingestion pipeline first.\")\n",
    "if performance_df is None:\n",
    "    print(\"Loan performance table not found. Run the data ingestion pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Schema and Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to examine dataframe structure and stats\n",
    "def examine_dataframe(df, name):\n",
    "    if df is None:\n",
    "        print(f\"{name} dataframe is not available\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Examining {name} Dataset\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Number of records: {df.count()}\")\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "    print(\"\\nSchema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nSample data:\")\n",
    "    df.show(5, truncate=False)\n",
    "    \n",
    "    # Calculate null counts\n",
    "    null_counts = []\n",
    "    for column in df.columns:\n",
    "        null_count = df.filter(col(column).isNull() | isnan(column)).count()\n",
    "        null_percent = (null_count / df.count()) * 100 if df.count() > 0 else 0\n",
    "        null_counts.append((column, null_count, null_percent))\n",
    "    \n",
    "    print(\"\\nNull value analysis:\")\n",
    "    for column, count, percent in null_counts:\n",
    "        if count > 0:\n",
    "            print(f\"{column}: {count} nulls ({percent:.2f}%)\")\n",
    "            \n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine each dataset\n",
    "app_nulls = examine_dataframe(applications_df, \"Loan Applications\")\n",
    "cust_nulls = examine_dataframe(customers_df, \"Customers\")\n",
    "perf_nulls = examine_dataframe(performance_df, \"Loan Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Numerical Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to analyze numerical features\n",
    "def analyze_numerical_features(df, features, name):\n",
    "    if df is None:\n",
    "        print(f\"{name} dataframe is not available\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Numerical Feature Analysis for {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Calculate statistics for each numerical feature\n",
    "    stats = {}\n",
    "    for feature in features:\n",
    "        if feature in df.columns:\n",
    "            stats_df = df.select(\n",
    "                min(feature).alias(\"min\"),\n",
    "                max(feature).alias(\"max\"),\n",
    "                mean(feature).alias(\"mean\"),\n",
    "                stddev(feature).alias(\"stddev\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            stats[feature] = {\n",
    "                \"min\": stats_df[\"min\"],\n",
    "                \"max\": stats_df[\"max\"],\n",
    "                \"mean\": stats_df[\"mean\"],\n",
    "                \"stddev\": stats_df[\"stddev\"]\n",
    "            }\n",
    "    \n",
    "    # Display statistics\n",
    "    for feature, stat in stats.items():\n",
    "        print(f\"\\nFeature: {feature}\")\n",
    "        for metric, value in stat.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    # Convert to Pandas for visualization\n",
    "    pd_df = df.select([col(f) for f in features if f in df.columns]).toPandas()\n",
    "    \n",
    "    # Create histograms for each numerical feature\n",
    "    fig, axs = plt.subplots(len(pd_df.columns), 1, figsize=(12, 4*len(pd_df.columns)))\n",
    "    \n",
    "    for i, feature in enumerate(pd_df.columns):\n",
    "        if len(pd_df.columns) > 1:\n",
    "            ax = axs[i]\n",
    "        else:\n",
    "            ax = axs\n",
    "            \n",
    "        pd_df[feature].hist(bins=30, ax=ax)\n",
    "        ax.set_title(f\"{feature} Distribution\")\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return stats, pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define numerical features for each dataset\n",
    "app_num_features = [\"loan_amount\", \"loan_term\", \"interest_rate\", \"annual_income\", \"debt_to_income_ratio\"]\n",
    "cust_num_features = [\"age\", \"num_credit_lines\", \"num_late_payments_30d\", \"num_late_payments_60d\", \n",
    "                     \"num_late_payments_90d\", \"credit_score\", \"utilization_rate\", \"num_inquiries_6m\"]\n",
    "perf_num_features = [\"current_balance\", \"remaining_payments\", \"days_past_due\"]\n",
    "\n",
    "# Analyze numerical features for each dataset\n",
    "app_num_stats, app_num_df = analyze_numerical_features(applications_df, app_num_features, \"Loan Applications\")\n",
    "cust_num_stats, cust_num_df = analyze_numerical_features(customers_df, cust_num_features, \"Customers\")\n",
    "perf_num_stats, perf_num_df = analyze_numerical_features(performance_df, perf_num_features, \"Loan Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Categorical Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to analyze categorical features\n",
    "def analyze_categorical_features(df, features, name):\n",
    "    if df is None:\n",
    "        print(f\"{name} dataframe is not available\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Categorical Feature Analysis for {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Calculate value counts for each categorical feature\n",
    "    value_counts = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature in df.columns:\n",
    "            counts = df.groupBy(feature).count().orderBy(\"count\", ascending=False).collect()\n",
    "            value_counts[feature] = [(row[feature], row[\"count\"]) for row in counts]\n",
    "    \n",
    "    # Display value counts\n",
    "    for feature, counts in value_counts.items():\n",
    "        print(f\"\\nFeature: {feature}\")\n",
    "        for value, count in counts:\n",
    "            print(f\"  {value}: {count}\")\n",
    "    \n",
    "    # Create bar plots for visualizing categorical distributions\n",
    "    for feature, counts in value_counts.items():\n",
    "        # Convert to pandas for easier plotting\n",
    "        pd_counts = pd.DataFrame(counts, columns=[feature, 'count'])\n",
    "        \n",
    "        # Limit to top 10 categories if there are many\n",
    "        if len(pd_counts) > 10:\n",
    "            pd_counts = pd_counts.sort_values('count', ascending=False).head(10)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(pd_counts[feature].astype(str), pd_counts['count'])\n",
    "        plt.title(f\"{feature} Distribution (Top 10)\")\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define categorical features for each dataset\n",
    "app_cat_features = [\"loan_purpose\", \"loan_grade\", \"employment_status\", \"home_ownership\", \n",
    "                    \"verification_status\", \"application_type\", \"state\"]\n",
    "perf_cat_features = [\"loan_status\", \"default_status\"]\n",
    "\n",
    "# Analyze categorical features for each dataset\n",
    "app_cat_counts = analyze_categorical_features(applications_df, app_cat_features, \"Loan Applications\")\n",
    "perf_cat_counts = analyze_categorical_features(performance_df, perf_cat_features, \"Loan Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze the target variable (default_status)\n",
    "if performance_df is not None:\n",
    "    target_counts = performance_df.groupBy(TARGET_COLUMN).count().collect()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Target Variable Analysis: {TARGET_COLUMN}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    total = performance_df.count()\n",
    "    for row in target_counts:\n",
    "        label = row[TARGET_COLUMN]\n",
    "        count = row[\"count\"]\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"Class {label}: {count} records ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    pd_target = pd.DataFrame([(row[TARGET_COLUMN], row[\"count\"]) for row in target_counts], \n",
    "                           columns=[TARGET_COLUMN, 'count'])\n",
    "    \n",
    "    # Plot target distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(pd_target[TARGET_COLUMN].astype(str), pd_target['count'])\n",
    "    plt.title(f\"{TARGET_COLUMN} Distribution\")\n",
    "    plt.xlabel(TARGET_COLUMN)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check class imbalance\n",
    "    if len(target_counts) > 1:\n",
    "        imbalance_ratio = max([row[\"count\"] for row in target_counts]) / min([row[\"count\"] for row in target_counts])\n",
    "        print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "        \n",
    "        if imbalance_ratio > 10:\n",
    "            print(\"Warning: Severe class imbalance detected. Consider using class weights, oversampling, or undersampling.\")\n",
    "        elif imbalance_ratio > 3:\n",
    "            print(\"Note: Moderate class imbalance detected. Consider handling during model training.\")\n",
    "else:\n",
    "    print(\"Loan performance data with target variable not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Correlations and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Join datasets for correlation analysis\n",
    "if applications_df is not None and customers_df is not None and performance_df is not None:\n",
    "    # Join loan applications with customer data\n",
    "    joined_df = applications_df.join(\n",
    "        customers_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Join with loan performance data\n",
    "    full_df = joined_df.join(\n",
    "        performance_df,\n",
    "        on=[\"application_id\", \"customer_id\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Joined dataset has {full_df.count()} records\")\n",
    "    \n",
    "    # Convert to pandas for correlation analysis\n",
    "    # Select numerical features and target\n",
    "    corr_features = [\n",
    "        \"loan_amount\", \"interest_rate\", \"annual_income\", \"debt_to_income_ratio\",\n",
    "        \"age\", \"credit_score\", \"num_credit_lines\", \"num_late_payments_30d\", \n",
    "        \"num_late_payments_60d\", \"num_late_payments_90d\", \"utilization_rate\", \n",
    "        TARGET_COLUMN\n",
    "    ]\n",
    "    \n",
    "    # Select only features that exist in the dataframe\n",
    "    existing_features = [f for f in corr_features if f in full_df.columns]\n",
    "    \n",
    "    # Convert to pandas (limit to 10,000 rows for memory efficiency in notebook)\n",
    "    pd_full = full_df.select(existing_features).limit(10000).toPandas()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = pd_full.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature correlation with target variable\n",
    "    if TARGET_COLUMN in pd_full.columns:\n",
    "        target_corr = corr_matrix[TARGET_COLUMN].sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Feature Correlations with {TARGET_COLUMN}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(target_corr)\n",
    "        \n",
    "        # Plot top correlated features\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        target_corr = target_corr.drop(TARGET_COLUMN)  # Remove self-correlation\n",
    "        target_corr.plot(kind='barh')\n",
    "        plt.title(f'Feature Correlation with {TARGET_COLUMN}')\n",
    "        plt.xlabel('Correlation Coefficient')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"One or more required datasets not available for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relationship Between Categorical Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze relationship between categorical features and target\n",
    "if 'full_df' in locals():\n",
    "    categorical_features = [\n",
    "        \"loan_purpose\", \"loan_grade\", \"employment_status\", \"home_ownership\",\n",
    "        \"verification_status\", \"application_type\", \"state\"\n",
    "    ]\n",
    "    \n",
    "    # Select only features that exist in the dataframe\n",
    "    existing_cat_features = [f for f in categorical_features if f in full_df.columns]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Categorical Features vs {TARGET_COLUMN}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for feature in existing_cat_features:\n",
    "        # Calculate default rate by category\n",
    "        category_stats = full_df.groupBy(feature).agg(\n",
    "            count(\"*\").alias(\"total\"),\n",
    "            sum(TARGET_COLUMN).alias(\"defaults\"),\n",
    "            (sum(TARGET_COLUMN) / count(\"*\") * 100).alias(\"default_rate\")\n",
    "        ).orderBy(\"default_rate\", ascending=False)\n",
    "        \n",
    "        # Convert to pandas for visualization\n",
    "        pd_cats = category_stats.toPandas()\n",
    "        \n",
    "        print(f\"\\nFeature: {feature}\")\n",
    "        for _, row in pd_cats.iterrows():\n",
    "            print(f\"  {row[feature]}: {row['default_rate']:.2f}% default rate ({row['defaults']} defaults out of {row['total']} loans)\")\n",
    "        \n",
    "        # Plot default rate by category\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=feature, y='default_rate', data=pd_cats)\n",
    "        plt.title(f'Default Rate by {feature}')\n",
    "        plt.ylabel('Default Rate (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Joined dataset not available for categorical analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Insights and Recommendations\n",
    "\n",
    "Based on the exploratory data analysis, here are some key insights and recommendations for model development:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "- [Document your observations about missing values]\n",
    "- [Document your observations about outliers]\n",
    "- [Document any data quality issues to address]\n",
    "\n",
    "### Feature Engineering Ideas\n",
    "- [List feature engineering ideas based on your analysis]\n",
    "- [Identify potential interaction terms]\n",
    "- [Suggest transformations for skewed distributions]\n",
    "\n",
    "### Model Development Recommendations\n",
    "- [Document class imbalance handling approach]\n",
    "- [Suggest potential algorithms based on the data characteristics]\n",
    "- [Identify potential evaluation metrics]\n",
    "\n",
    "### Next Steps\n",
    "- Develop a robust feature engineering pipeline\n",
    "- Implement data cleaning and preparation steps\n",
    "- Develop and test multiple modeling approaches\n",
    "- Create a risk scoring system based on model outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
