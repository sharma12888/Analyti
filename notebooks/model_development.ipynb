{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Score Analysis - Model Development\n",
    "\n",
    "This notebook demonstrates the process of developing and evaluating machine learning models for risk scoring. We'll explore different algorithms, hyperparameters, and evaluation metrics to find the best model for our risk scoring pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, rand\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Scikit-learn and XGBoost imports\n",
    "from sklearn.ensemble import RandomForestClassifier as SklearnRF\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import (\n",
    "    DELTA_TABLES,\n",
    "    MODEL_PATH,\n",
    "    RANDOM_SEED,\n",
    "    TRAIN_TEST_SPLIT_RATIO,\n",
    "    VALIDATION_SPLIT_RATIO,\n",
    "    TARGET_COLUMN,\n",
    "    CURRENT_DATE\n",
    ")\n",
    "from src.utils import (\n",
    "    get_spark_session, \n",
    "    read_delta_table,\n",
    "    setup_mlflow,\n",
    "    save_model_artifacts\n",
    ")\n",
    "from src.feature_engineering import FeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Spark Session\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Set up MLflow for tracking experiments\n",
    "setup_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Load the feature-engineered data from Delta Lake and prepare it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load feature data from Delta Lake\n",
    "feature_df = read_delta_table(spark, DELTA_TABLES[\"feature_table\"])\n",
    "\n",
    "if feature_df is None or feature_df.rdd.isEmpty():\n",
    "    print(\"Feature table is empty or does not exist. Running feature engineering pipeline.\")\n",
    "    # Initialize feature engineering\n",
    "    feature_engineer = FeatureEngineer(spark)\n",
    "    # Run feature engineering pipeline\n",
    "    feature_df = feature_engineer.run_feature_engineering_pipeline()\n",
    "    \n",
    "print(f\"Loaded {feature_df.count()} records with {len(feature_df.columns)} features\")\n",
    "\n",
    "# Show feature dataframe schema\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare features for modeling\n",
    "feature_engineer = FeatureEngineer(spark)\n",
    "prepared_df, pipeline_model = feature_engineer.prepare_features_for_modeling(feature_df)\n",
    "\n",
    "print(f\"Prepared dataframe has {prepared_df.count()} records\")\n",
    "prepared_df.select(\"features\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training, validation, and test sets\n",
    "# First split: train + validation and test\n",
    "train_val_df, test_df = prepared_df.randomSplit(\n",
    "    [1 - TRAIN_TEST_SPLIT_RATIO, TRAIN_TEST_SPLIT_RATIO],\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Second split: train and validation\n",
    "train_ratio = 1 - (VALIDATION_SPLIT_RATIO / (1 - TRAIN_TEST_SPLIT_RATIO))\n",
    "train_df, val_df = train_val_df.randomSplit(\n",
    "    [train_ratio, 1 - train_ratio],\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Data split: Train {train_df.count()}, Validation {val_df.count()}, Test {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Development and Evaluation\n",
    "\n",
    "In this section, we'll train and evaluate different models for risk scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=TARGET_COLUMN,\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8,\n",
    "    standardization=True,\n",
    "    family=\"binomial\"\n",
    ")\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.3]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 0.8, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    labelCol=TARGET_COLUMN,\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Train model with cross-validation\n",
    "print(\"Training logistic regression model...\")\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Get best model\n",
    "lr_model = cv_model.bestModel\n",
    "\n",
    "# Get parameters of the best model\n",
    "print(f\"Best model parameters:\")\n",
    "print(f\"  Regularization parameter: {lr_model.getRegParam()}\")\n",
    "print(f\"  Elastic Net parameter: {lr_model.getElasticNetParam()}\")\n",
    "print(f\"  Max iterations: {lr_model.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate logistic regression model on validation set\n",
    "lr_predictions = lr_model.transform(val_df)\n",
    "lr_auc = evaluator.evaluate(lr_predictions)\n",
    "\n",
    "# Calculate additional metrics\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=TARGET_COLUMN,\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "lr_accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(lr_predictions)\n",
    "lr_precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(lr_predictions)\n",
    "lr_recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(lr_predictions)\n",
    "lr_f1 = multi_evaluator.setMetricName(\"f1\").evaluate(lr_predictions)\n",
    "\n",
    "print(\"Logistic Regression Evaluation Metrics:\")\n",
    "print(f\"  AUC: {lr_auc:.4f}\")\n",
    "print(f\"  Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"  Precision: {lr_precision:.4f}\")\n",
    "print(f\"  Recall: {lr_recall:.4f}\")\n",
    "print(f\"  F1 Score: {lr_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=TARGET_COLUMN,\n",
    "    predictionCol=\"prediction\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    maxBins=32,\n",
    "    minInstancesPerNode=1,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,  # reuse evaluator from above\n",
    "    numFolds=3,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Train model with cross-validation\n",
    "print(\"Training random forest model...\")\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Get best model\n",
    "rf_model = cv_model.bestModel\n",
    "\n",
    "# Get parameters of the best model\n",
    "print(f\"Best model parameters:\")\n",
    "print(f\"  Number of trees: {rf_model.getNumTrees()}\")\n",
    "print(f\"  Max depth: {rf_model.getMaxDepth()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate Random Forest model on validation set\n",
    "rf_predictions = rf_model.transform(val_df)\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# Calculate additional metrics\n",
    "rf_accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(rf_predictions)\n",
    "rf_precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(rf_predictions)\n",
    "rf_recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(rf_predictions)\n",
    "rf_f1 = multi_evaluator.setMetricName(\"f1\").evaluate(rf_predictions)\n",
    "\n",
    "print(\"Random Forest Evaluation Metrics:\")\n",
    "print(f\"  AUC: {rf_auc:.4f}\")\n",
    "print(f\"  Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"  Precision: {rf_precision:.4f}\")\n",
    "print(f\"  Recall: {rf_recall:.4f}\")\n",
    "print(f\"  F1 Score: {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance from Random Forest model\n",
    "feature_importances = rf_model.featureImportances\n",
    "print(\"Feature importances:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# TODO: If feature names are available, map feature importances to feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert data to format for XGBoost (Spark DataFrame to pandas with numpy arrays)\n",
    "train_pd = train_df.select(\"features\", TARGET_COLUMN).toPandas()\n",
    "val_pd = val_df.select(\"features\", TARGET_COLUMN).toPandas()\n",
    "\n",
    "# Extract features and target\n",
    "X_train = np.array([x.toArray() for x in train_pd[\"features\"]])\n",
    "y_train = train_pd[TARGET_COLUMN].values\n",
    "\n",
    "X_val = np.array([x.toArray() for x in val_pd[\"features\"]])\n",
    "y_val = val_pd[TARGET_COLUMN].values\n",
    "\n",
    "print(f\"XGBoost training data: {X_train.shape[0]} samples with {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 100,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": RANDOM_SEED\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate XGBoost model on validation set\n",
    "y_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "xgb_accuracy = accuracy_score(y_val, y_pred)\n",
    "xgb_precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "xgb_recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "xgb_f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "\n",
    "print(\"XGBoost Evaluation Metrics:\")\n",
    "print(f\"  AUC: {xgb_auc:.4f}\")\n",
    "print(f\"  Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"  Precision: {xgb_precision:.4f}\")\n",
    "print(f\"  Recall: {xgb_recall:.4f}\")\n",
    "print(f\"  F1 Score: {xgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot feature importances from XGBoost\n",
    "plt.figure(figsize=(12, 8))\n",
    "xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "plt.title('XGBoost Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison and Selection\n",
    "\n",
    "Compare the performance of different models and select the best one for risk scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compile validation metrics for all models\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'auc': lr_auc,\n",
    "        'accuracy': lr_accuracy,\n",
    "        'precision': lr_precision,\n",
    "        'recall': lr_recall,\n",
    "        'f1': lr_f1\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'auc': rf_auc,\n",
    "        'accuracy': rf_accuracy,\n",
    "        'precision': rf_precision,\n",
    "        'recall': rf_recall,\n",
    "        'f1': rf_f1\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'auc': xgb_auc,\n",
    "        'accuracy': xgb_accuracy,\n",
    "        'precision': xgb_precision,\n",
    "        'recall': xgb_recall,\n",
    "        'f1': xgb_f1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "model_comparison = pd.DataFrame(models).T\n",
    "print(\"Model Comparison on Validation Set:\")\n",
    "display(model_comparison)\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "model_comparison.plot(kind='bar')\n",
    "plt.title('Model Comparison on Validation Set')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select the best model based on AUC\n",
    "best_model_name = model_comparison['auc'].idxmax()\n",
    "best_auc = model_comparison.loc[best_model_name, 'auc']\n",
    "\n",
    "print(f\"Best model based on AUC: {best_model_name} with AUC = {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Evaluation on Test Set\n",
    "\n",
    "Evaluate the best model on the holdout test set to get a final assessment of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate models on test set\n",
    "print(\"Evaluating models on test set...\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_test_predictions = lr_model.transform(test_df)\n",
    "lr_test_auc = evaluator.evaluate(lr_test_predictions)\n",
    "lr_test_accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(lr_test_predictions)\n",
    "lr_test_precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(lr_test_predictions)\n",
    "lr_test_recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(lr_test_predictions)\n",
    "lr_test_f1 = multi_evaluator.setMetricName(\"f1\").evaluate(lr_test_predictions)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_test_predictions = rf_model.transform(test_df)\n",
    "rf_test_auc = evaluator.evaluate(rf_test_predictions)\n",
    "rf_test_accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(rf_test_predictions)\n",
    "rf_test_precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(rf_test_predictions)\n",
    "rf_test_recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(rf_test_predictions)\n",
    "rf_test_f1 = multi_evaluator.setMetricName(\"f1\").evaluate(rf_test_predictions)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "test_pd = test_df.select(\"features\", TARGET_COLUMN).toPandas()\n",
    "X_test = np.array([x.toArray() for x in test_pd[\"features\"]])\n",
    "y_test = test_pd[TARGET_COLUMN].values\n",
    "\n",
    "xgb_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "xgb_test_auc = roc_auc_score(y_test, xgb_test_pred_proba)\n",
    "xgb_test_accuracy = accuracy_score(y_test, xgb_test_pred)\n",
    "xgb_test_precision = precision_score(y_test, xgb_test_pred, zero_division=0)\n",
    "xgb_test_recall = recall_score(y_test, xgb_test_pred, zero_division=0)\n",
    "xgb_test_f1 = f1_score(y_test, xgb_test_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compile test metrics for all models\n",
    "test_metrics = {\n",
    "    'Logistic Regression': {\n",
    "        'auc': lr_test_auc,\n",
    "        'accuracy': lr_test_accuracy,\n",
    "        'precision': lr_test_precision,\n",
    "        'recall': lr_test_recall,\n",
    "        'f1': lr_test_f1\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'auc': rf_test_auc,\n",
    "        'accuracy': rf_test_accuracy,\n",
    "        'precision': rf_test_precision,\n",
    "        'recall': rf_test_recall,\n",
    "        'f1': rf_test_f1\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'auc': xgb_test_auc,\n",
    "        'accuracy': xgb_test_accuracy,\n",
    "        'precision': xgb_test_precision,\n",
    "        'recall': xgb_test_recall,\n",
    "        'f1': xgb_test_f1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_comparison = pd.DataFrame(test_metrics).T\n",
    "print(\"Model Comparison on Test Set:\")\n",
    "display(test_comparison)\n",
    "\n",
    "# Plot model comparison on test set\n",
    "plt.figure(figsize=(12, 8))\n",
    "test_comparison.plot(kind='bar')\n",
    "plt.title('Model Comparison on Test Set')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select the final best model based on test AUC\n",
    "final_best_model_name = test_comparison['auc'].idxmax()\n",
    "final_best_auc = test_comparison.loc[final_best_model_name, 'auc']\n",
    "\n",
    "print(f\"Final best model based on test AUC: {final_best_model_name} with AUC = {final_best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC and Precision-Recall Curves\n",
    "\n",
    "Plot ROC and Precision-Recall curves for the models to visualize their performance across different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# XGBoost ROC curve\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_test_pred_proba)\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {xgb_test_auc:.4f})')\n",
    "\n",
    "# For Spark models, convert predictions to pandas\n",
    "lr_preds_pd = lr_test_predictions.select(TARGET_COLUMN, \"probability\").toPandas()\n",
    "lr_probs = np.array([p[1] for p in lr_preds_pd[\"probability\"]])\n",
    "lr_true = lr_preds_pd[TARGET_COLUMN].values\n",
    "\n",
    "rf_preds_pd = rf_test_predictions.select(TARGET_COLUMN, \"probability\").toPandas()\n",
    "rf_probs = np.array([p[1] for p in rf_preds_pd[\"probability\"]])\n",
    "rf_true = rf_preds_pd[TARGET_COLUMN].values\n",
    "\n",
    "# Logistic Regression ROC curve\n",
    "fpr_lr, tpr_lr, _ = roc_curve(lr_true, lr_probs)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {lr_test_auc:.4f})')\n",
    "\n",
    "# Random Forest ROC curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(rf_true, rf_probs)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_test_auc:.4f})')\n",
    "\n",
    "# Add diagonal line for reference (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Different Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot Precision-Recall curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# XGBoost PR curve\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, xgb_test_pred_proba)\n",
    "pr_auc_xgb = auc(recall_xgb, precision_xgb)\n",
    "plt.plot(recall_xgb, precision_xgb, label=f'XGBoost (AUC = {pr_auc_xgb:.4f})')\n",
    "\n",
    "# Logistic Regression PR curve\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(lr_true, lr_probs)\n",
    "pr_auc_lr = auc(recall_lr, precision_lr)\n",
    "plt.plot(recall_lr, precision_lr, label=f'Logistic Regression (AUC = {pr_auc_lr:.4f})')\n",
    "\n",
    "# Random Forest PR curve\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(rf_true, rf_probs)\n",
    "pr_auc_rf = auc(recall_rf, precision_rf)\n",
    "plt.plot(recall_rf, precision_rf, label=f'Random Forest (AUC = {pr_auc_rf:.4f})')\n",
    "\n",
    "# Add baseline for reference (ratio of positives)\n",
    "baseline = np.sum(y_test) / len(y_test)\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.4f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves for Different Models')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix\n",
    "\n",
    "Examine the confusion matrix for the best model to understand its predictions in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create confusion matrix for the best model\n",
    "if final_best_model_name == 'Logistic Regression':\n",
    "    best_preds = lr_test_predictions.select(TARGET_COLUMN, \"prediction\").toPandas()\n",
    "    y_true = best_preds[TARGET_COLUMN].values\n",
    "    y_pred = best_preds[\"prediction\"].values\n",
    "elif final_best_model_name == 'Random Forest':\n",
    "    best_preds = rf_test_predictions.select(TARGET_COLUMN, \"prediction\").toPandas()\n",
    "    y_true = best_preds[TARGET_COLUMN].values\n",
    "    y_pred = best_preds[\"prediction\"].values\n",
    "else:  # XGBoost\n",
    "    y_true = y_test\n",
    "    y_pred = xgb_test_pred\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Default', 'Default'],\n",
    "            yticklabels=['Non-Default', 'Default'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix - {final_best_model_name}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate (Recall)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0    # Precision\n",
    "npv = tn / (tn + fn) if (tn + fn) > 0 else 0          # Negative Predictive Value\n",
    "\n",
    "print(f\"Confusion Matrix Metrics for {final_best_model_name}:\")\n",
    "print(f\"  True Negatives: {tn}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  True Positives: {tp}\")\n",
    "print(f\"  Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"  Specificity: {specificity:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Negative Predictive Value: {npv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Best Model\n",
    "\n",
    "Save the best model and pipeline for use in the risk scoring system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the best model\n",
    "print(f\"Saving best model: {final_best_model_name}\")\n",
    "\n",
    "# Get the model object and type\n",
    "if final_best_model_name == 'Logistic Regression':\n",
    "    best_model = lr_model\n",
    "    model_type = 'spark_lr'\n",
    "    test_metrics = test_metrics['Logistic Regression']\n",
    "elif final_best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "    model_type = 'spark_rf'\n",
    "    test_metrics = test_metrics['Random Forest']\n",
    "else:  # XGBoost\n",
    "    best_model = xgb_model\n",
    "    model_type = 'xgboost'\n",
    "    test_metrics = test_metrics['XGBoost']\n",
    "\n",
    "# Get feature names\n",
    "if 'feature_df' in locals():\n",
    "    feature_names = [col for col in feature_df.columns \n",
    "                   if col != TARGET_COLUMN]\n",
    "else:\n",
    "    feature_names = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "# Define model parameters\n",
    "if model_type == 'spark_lr':\n",
    "    params = {\n",
    "        \"maxIter\": best_model.getMaxIter(),\n",
    "        \"regParam\": best_model.getRegParam(),\n",
    "        \"elasticNetParam\": best_model.getElasticNetParam()\n",
    "    }\n",
    "elif model_type == 'spark_rf':\n",
    "    params = {\n",
    "        \"numTrees\": best_model.getNumTrees(),\n",
    "        \"maxDepth\": best_model.getMaxDepth(),\n",
    "        \"maxBins\": best_model.getMaxBins()\n",
    "    }\n",
    "elif model_type == 'xgboost':\n",
    "    params = {\n",
    "        \"max_depth\": best_model.get_params()['max_depth'],\n",
    "        \"learning_rate\": best_model.get_params()['learning_rate'],\n",
    "        \"n_estimators\": best_model.get_params()['n_estimators'],\n",
    "        \"subsample\": best_model.get_params()['subsample'],\n",
    "        \"colsample_bytree\": best_model.get_params()['colsample_bytree']\n",
    "    }\n",
    "\n",
    "# Add common parameters\n",
    "params.update({\n",
    "    \"model_type\": model_type,\n",
    "    \"training_date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "    \"random_seed\": RANDOM_SEED\n",
    "})\n",
    "\n",
    "# Create model name with timestamp\n",
    "model_name = f\"{model_type}_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Save model artifacts\n",
    "model_path = save_model_artifacts(\n",
    "    model=best_model,\n",
    "    model_name=model_name,\n",
    "    model_type=model_type,\n",
    "    features=feature_names,\n",
    "    metrics=test_metrics,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# Save pipeline model if available\n",
    "if 'pipeline_model' in locals() and pipeline_model is not None:\n",
    "    pipeline_path = os.path.join(model_path, \"pipeline_model\")\n",
    "    pipeline_model.write().overwrite().save(pipeline_path)\n",
    "    print(f\"Saved pipeline model to {pipeline_path}\")\n",
    "\n",
    "print(f\"Best model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Risk Score Generation\n",
    "\n",
    "Demonstrate how the model can be used to generate risk scores for loan applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define risk score ranges\n",
    "RISK_SCORE_RANGES = {\n",
    "    \"very_low\": (0, 20),\n",
    "    \"low\": (21, 40),\n",
    "    \"medium\": (41, 60),\n",
    "    \"high\": (61, 80),\n",
    "    \"very_high\": (81, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate risk scores for a sample of test data\n",
    "if model_type.startswith('spark'):\n",
    "    # Use Spark model to make predictions\n",
    "    risk_preds = best_model.transform(test_df.limit(100))\n",
    "    \n",
    "    # Extract probability of default (class 1)\n",
    "    from pyspark.sql.functions import udf, col, when, lit\n",
    "    from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "    \n",
    "    # Extract default probability and calculate risk score\n",
    "    risk_df = risk_preds.withColumn(\n",
    "        \"default_probability\", \n",
    "        risk_preds[\"probability\"].getItem(1)\n",
    "    ).withColumn(\n",
    "        \"risk_score\",\n",
    "        (col(\"default_probability\") * 100).cast(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Add risk category\n",
    "    risk_df = risk_df.withColumn(\n",
    "        \"risk_category\",\n",
    "        when(col(\"risk_score\").between(RISK_SCORE_RANGES[\"very_low\"][0], RISK_SCORE_RANGES[\"very_low\"][1]), \"very_low\")\n",
    "        .when(col(\"risk_score\").between(RISK_SCORE_RANGES[\"low\"][0], RISK_SCORE_RANGES[\"low\"][1]), \"low\")\n",
    "        .when(col(\"risk_score\").between(RISK_SCORE_RANGES[\"medium\"][0], RISK_SCORE_RANGES[\"medium\"][1]), \"medium\")\n",
    "        .when(col(\"risk_score\").between(RISK_SCORE_RANGES[\"high\"][0], RISK_SCORE_RANGES[\"high\"][1]), \"high\")\n",
    "        .when(col(\"risk_score\").between(RISK_SCORE_RANGES[\"very_high\"][0], RISK_SCORE_RANGES[\"very_high\"][1]), \"very_high\")\n",
    "        .otherwise(\"unknown\")\n",
    "    )\n",
    "    \n",
    "    # Display sample risk scores\n",
    "    display(risk_df.select(\n",
    "        \"default_probability\", \"risk_score\", \"risk_category\", TARGET_COLUMN, \"prediction\"\n",
    "    ).limit(10))\n",
    "    \n",
    "    # Create distribution of risk categories\n",
    "    risk_dist = risk_df.groupBy(\"risk_category\").count().orderBy(\"risk_category\")\n",
    "    risk_dist_pd = risk_dist.toPandas()\n",
    "    \n",
    "    # Plot distribution of risk categories\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(risk_dist_pd[\"risk_category\"], risk_dist_pd[\"count\"])\n",
    "    plt.title('Distribution of Risk Categories')\n",
    "    plt.xlabel('Risk Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:  # XGBoost model\n",
    "    # Use scikit-learn/XGBoost model to make predictions\n",
    "    sample_X = X_test[:100]\n",
    "    sample_y = y_test[:100]\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    sample_preds = best_model.predict(sample_X)\n",
    "    sample_probs = best_model.predict_proba(sample_X)[:, 1]\n",
    "    \n",
    "    # Calculate risk scores\n",
    "    risk_scores = (sample_probs * 100).astype(int)\n",
    "    \n",
    "    # Assign risk categories\n",
    "    def get_risk_category(score):\n",
    "        if RISK_SCORE_RANGES[\"very_low\"][0] <= score <= RISK_SCORE_RANGES[\"very_low\"][1]:\n",
    "            return \"very_low\"\n",
    "        elif RISK_SCORE_RANGES[\"low\"][0] <= score <= RISK_SCORE_RANGES[\"low\"][1]:\n",
    "            return \"low\"\n",
    "        elif RISK_SCORE_RANGES[\"medium\"][0] <= score <= RISK_SCORE_RANGES[\"medium\"][1]:\n",
    "            return \"medium\"\n",
    "        elif RISK_SCORE_RANGES[\"high\"][0] <= score <= RISK_SCORE_RANGES[\"high\"][1]:\n",
    "            return \"high\"\n",
    "        elif RISK_SCORE_RANGES[\"very_high\"][0] <= score <= RISK_SCORE_RANGES[\"very_high\"][1]:\n",
    "            return \"very_high\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    risk_categories = [get_risk_category(score) for score in risk_scores]\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    risk_results = pd.DataFrame({\n",
    "        \"default_probability\": sample_probs,\n",
    "        \"risk_score\": risk_scores,\n",
    "        \"risk_category\": risk_categories,\n",
    "        TARGET_COLUMN: sample_y,\n",
    "        \"prediction\": sample_preds\n",
    "    })\n",
    "    \n",
    "    # Display sample risk scores\n",
    "    display(risk_results.head(10))\n",
    "    \n",
    "    # Plot distribution of risk categories\n",
    "    risk_dist = risk_results[\"risk_category\"].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    risk_dist.plot(kind='bar')\n",
    "    plt.title('Distribution of Risk Categories')\n",
    "    plt.xlabel('Risk Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "Based on our model development and evaluation, here are the key findings and next steps for the risk scoring system:\n",
    "\n",
    "### Key Findings\n",
    "- [Document the best performing model and its metrics]\n",
    "- [Summarize important features for risk prediction]\n",
    "- [Describe the distribution of risk scores in the dataset]\n",
    "\n",
    "### Next Steps\n",
    "1. Implement the full risk scoring pipeline using the best model\n",
    "2. Create monitoring and retraining processes for the model\n",
    "3. Develop a system to explain risk scores and provide reasons\n",
    "4. Validate the risk scoring system on new data\n",
    "\n",
    "### Potential Improvements\n",
    "- [List potential improvements to the model or pipeline]\n",
    "- [Suggest additional features or data sources]\n",
    "- [Recommend model deployment and monitoring strategies]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
